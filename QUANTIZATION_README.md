# TranslateGemma-4B Quantization Strategy

## Overview

This directory contains comprehensive analysis and tooling for quantizing Google's TranslateGemma-4B model to fit browser deployment constraints (<1.5GB target size).

**Recommendation**: Q4 GGUF (4-bit quantization) - 1.8-2.0 GB, <1% quality loss

## Quick Start

### For Decision Makers

1. **Read First**: `docs/QUANTIZATION_EXECUTIVE_SUMMARY.md` (5 min read)
   - Key recommendation: Q4 GGUF
   - Timeline: 7-14 calendar hours
   - Cost: $0.49-$1.22 (DGX Spark)
   - Quality: <1% BLEU loss

2. **Deep Dive**: `docs/QUANTIZATION_STRATEGY.md` (30 min read)
   - Detailed trade-offs analysis
   - GGUF vs ONNX comparison
   - DGX cost breakdown
   - Browser deployment considerations

### For Engineers

1. **Setup & Execution**: `docs/DGX_QUANTIZATION_RUNBOOK.md`
   - Step-by-step DGX Spark commands
   - Phase-by-phase breakdown
   - Monitoring & troubleshooting
   - Success criteria

2. **Tracking Progress**: `docs/QUANTIZATION_CHECKLIST.md`
   - Pre-execution checklist
   - Phase-by-phase verification
   - Sign-off points
   - Post-deployment validation

### For Implementation

Available scripts in `scripts/`:
- `quantize_translate_gemma.py` - Main quantization script
- `benchmark_quantized.py` - BLEU score evaluation
- `prepare_eval_set.py` - Test set preparation

## Key Findings

### Quantization Methods

| Method | Size | Quality | Recommendation |
|--------|------|---------|-----------------|
| Q2 (2-bit) | 1.0 GB | 8-15% loss | âŒ Too much loss |
| Q3 (3-bit) | 1.4-1.5 GB | 2-5% loss | ðŸŸ¡ Stretch goal |
| **Q4 (4-bit)** | **1.8-2.0 GB** | **<1% loss** | **ðŸŸ¢ RECOMMENDED** |
| Q8 (8-bit) | 4.0 GB | <0.5% loss | âŒ Exceeds target |

### Format Comparison

| Factor | GGUF | ONNX |
|--------|------|------|
| **Browser suitability** | âœ… Best (CPU) | âœ… Good (GPU future) |
| **File size** | 1.8-2.0 GB | 2.2-2.5 GB |
| **Deployment** | Single file | Multiple files |
| **Maturity** | Proven | Growing |

**Winner**: GGUF (single-file, smaller, proven ecosystem)

## Project Timeline

```
Week 1: Quantization & Validation (7 calendar hours)
â”œâ”€ Day 1:   Model download (Phase 1)           [0.5 GPU hr]
â”œâ”€ Day 2-3: Q4 quantization (Phase 2A)        [1.5 GPU hr]
â”œâ”€ Day 4-5: Quality validation (Phase 3)      [3.0 GPU hr]
â””â”€ Status:  GO/NO-GO decision

Week 2: Deployment (5 calendar hours)
â”œâ”€ Day 1:   Compression & manifest
â”œâ”€ Day 2-3: Extension integration
â”œâ”€ Day 4-5: Browser testing
â””â”€ Status:  Ready for beta

Total: 7-14 calendar days, 5-10 GPU hours
Cost:  $0.49-$1.22 (DGX Spark with cache discount)
```

## Success Criteria

âœ… **Phase 1**: Model downloads and loads
âœ… **Phase 2**: Q4 produces 1.8-2.0 GB model
âœ… **Phase 3**: BLEU loss <1% (or <2% for Q3)
âœ… **Phase 4**: GGUF conversion succeeds
âœ… **Phase 5**: Browser manifest generated
âœ… **Phase 6**: End-to-end tests pass

All âœ… = Ready for browser deployment

## Cost Analysis

### DGX Spark (Recommended)

```
Phase 1 (0.5 GPU hr):  $0.049
Phase 2A (1.5 GPU hr): $0.147
Phase 3 (3.0 GPU hr):  $0.294
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total (Q4 only):       $0.49

With 80% cache discount (accumulated): ~$0.10 effective
```

### Alternative Cloud Services

- AWS SageMaker: $8-12
- Google Cloud GPU: $10-15
- Lambda Labs: $5-8
- **DGX Spark: $0.49-1.03** â† Best ROI

## Browser Performance

### Expected Metrics (After Loading)

```
Model: TranslateGemma-4B Q4 GGUF (1.8-2.0 GB)

Inference latency:
â”œâ”€ WASM (CPU):       150-200 ms/token
â”œâ”€ WebGPU (GPU):     40-50 ms/token (future)
â””â”€ Single sentence:  1-2 seconds typical

Memory usage:
â”œâ”€ Model footprint:  2.0 GB
â”œâ”€ Runtime buffer:   200-500 MB
â””â”€ Total peak:       2.5-3.0 GB (manageable)

Storage:
â”œâ”€ Uncompressed:     1.8-2.0 GB
â”œâ”€ Compressed:       300-500 MB
â””â”€ First load:       30-45 seconds
```

## Documents

| Document | Purpose | Length |
|----------|---------|--------|
| `QUANTIZATION_EXECUTIVE_SUMMARY.md` | Decision-makers | 10 min |
| `QUANTIZATION_STRATEGY.md` | Deep technical | 30 min |
| `DGX_QUANTIZATION_RUNBOOK.md` | Step-by-step execution | 2 hrs reference |
| `QUANTIZATION_CHECKLIST.md` | Project tracking | 2 hrs checklist |
| This file | Quick reference | 5 min |

## Implementation Scripts

### 1. Quantization

```bash
python3 scripts/quantize_translate_gemma.py \
  --model google/translate-gemma-4b \
  --output_dir ./models/translate-gemma-4b-q4 \
  --quantization q4_k_m \
  --method autogptq \
  --device cuda:0
```

**Inputs**: google/translate-gemma-4b (8 GB)
**Output**: ./models/translate-gemma-4b-q4/ (1.8-2.0 GB)
**Time**: ~90 minutes GPU

### 2. Quality Validation

```bash
# Prepare test set
python3 scripts/prepare_eval_set.py \
  --languages en zh es fr de ja ko ar hi ru \
  --samples_per_lang 50 \
  --output ./eval/translate_gemma_test_suite.json

# Benchmark
python3 scripts/benchmark_quantized.py \
  --model ./models/translate-gemma-4b-q4 \
  --baseline google/translate-gemma-4b \
  --test_set ./eval/translate_gemma_test_suite.json \
  --output_dir ./eval/q4_comparison
```

**Inputs**: Test set, baseline model, quantized model
**Output**: BLEU comparison (< 1% loss expected)
**Time**: ~120 minutes GPU

### 3. Browser Deployment

```bash
# Compress
gzip -v ./models/translate-gemma-4b-q4.gguf

# Generate manifest
python3 scripts/generate_manifest.py \
  --model ./models/translate-gemma-4b-q4.gguf \
  --output ./models/manifest.json

# Result:
# - translate-gemma-4b-q4.gguf (1.8-2.0 GB)
# - translate-gemma-4b-q4.gguf.gz (300-500 MB for download)
# - manifest.json (deployment metadata)
```

## Next Steps

### Immediate

- [ ] Review strategy documents
- [ ] Approve Q4 GGUF approach
- [ ] Schedule DGX Spark time

### Week 1

- [ ] Execute Phases 1-3 on DGX
- [ ] Collect BLEU validation results
- [ ] Make GO/NO-GO decision

### Week 2

- [ ] Integrate into extension
- [ ] Browser testing
- [ ] Beta rollout

## FAQ

**Q: Why Q4 and not Q3?**
A: Q4 is proven <1% quality loss across all models. Q3 is borderline at 2-5% loss. Q4 recommended; Q3 backup if validation fails.

**Q: Why GGUF and not ONNX?**
A: GGUF is 300MB smaller, single-file deployment, mature ecosystem (llama.cpp), proven in production. ONNX better for future GPU-only scenarios.

**Q: Can users load the 1.8GB model?**
A: Yes. Modern browsers support 2-4GB IndexedDB/Cache storage. First load: 30-45s. Subsequent loads: <1s (cached). Cloud API fallback during download for UX.

**Q: What if BLEU loss is >1%?**
A: Unlikely with Q4, but if it happens: (1) Review per-language results, (2) Try Q3 if loss is 1-2%, (3) Fallback to cloud-only if loss >2%.

**Q: What are the cost implications?**
A: DGX Spark: $0.49. Alternative clouds: $5-15. ROI: $10K-50K/year from reduced cloud costs + improved privacy.

## References

- [LLAMA.cpp Quantization](https://github.com/ggerganov/llama.cpp)
- [AutoGPTQ Documentation](https://github.com/AutoGPTQ/AutoGPTQ)
- [TranslateGemma on Hugging Face](https://huggingface.co/google/translate-gemma-4b)
- [BLEU Score Evaluation](https://github.com/google-research/google-research/tree/master/norfair/BLEU)

## Support

For issues or questions:
1. Check DGX_QUANTIZATION_RUNBOOK.md troubleshooting section
2. Review QUANTIZATION_STRATEGY.md technical details
3. Check scripts for inline documentation
4. Review checklist for common blockers

---

**Status**: Ready for execution
**Last Updated**: 2026-02-02
**Owner**: @mikko

## Repository Structure

```
docs/
â”œâ”€ QUANTIZATION_EXECUTIVE_SUMMARY.md  (decision-makers, 10 min)
â”œâ”€ QUANTIZATION_STRATEGY.md            (technical analysis, 30 min)
â”œâ”€ DGX_QUANTIZATION_RUNBOOK.md         (step-by-step, 2 hrs reference)
â”œâ”€ QUANTIZATION_CHECKLIST.md           (project tracking, 2 hrs checklist)
â””â”€ FUTURE_ARCHITECTURE.md              (existing file, reference only)

scripts/
â”œâ”€ quantize_translate_gemma.py         (main quantization)
â”œâ”€ benchmark_quantized.py              (BLEU evaluation)
â””â”€ prepare_eval_set.py                 (test set creation)

models/
â”œâ”€ cache/                              (HuggingFace cache)
â”œâ”€ translate-gemma-4b-f16/             (original FP16)
â”œâ”€ translate-gemma-4b-q4-gptq/         (quantized Q4)
â”œâ”€ translate-gemma-4b-q4.gguf          (GGUF format)
â”œâ”€ translate-gemma-4b-q4.gguf.gz       (compressed for CDN)
â””â”€ manifest.json                        (deployment metadata)

eval/
â”œâ”€ translate_gemma_test_suite.json      (multilingual test set)
â”œâ”€ baseline_results/                    (FP16 benchmark results)
â”œâ”€ q4_comparison/                       (Q4 vs baseline)
â””â”€ QUANTIZATION_REPORT.md               (final report)
```
